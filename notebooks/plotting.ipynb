{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f3a893",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'raas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPAgent\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiscretizedDPAgent\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulator, CustomerGenerator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'raas'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from raas.neural_policy import DPAgent\n",
    "from raas.discrete_policy import DiscretizedDPAgent\n",
    "from raas.simulation import Simulator, CustomerGenerator\n",
    "from raas.hazard_models import ExponentialHazard\n",
    "from raas.utility_learner import ProjectedVolumeLearner, diam\n",
    "from raas.degradation_learner import DegradationLearner\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "from raas.utils import unit_ball_sample, correct_signs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b922b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define Sampling Functions ---\n",
    "# def context_sampler() -> np.ndarray:\n",
    "#     \"\"\"Samples a customer's context vector from a uniform distribution.\"\"\"\n",
    "#     return np.random.uniform(low=0.0, high=1.0, size=D)\n",
    "\n",
    "def context_sampler() -> np.ndarray:\n",
    "    \"\"\"Samples a customer's context vector uniformly from the unit ball.\"\"\"\n",
    "    return np.abs(unit_ball_sample(D))\n",
    "\n",
    "def rental_sampler() -> float:\n",
    "    \"\"\"Samples a customer's desired rental duration from an exponential distribution.\"\"\"\n",
    "    return np.random.exponential(scale=10.0)\n",
    "\n",
    "def interarrival_sampler() -> float:\n",
    "    \"\"\"Samples the time until the next customer arrives.\"\"\"\n",
    "    return np.random.exponential(scale=5.0)\n",
    "\n",
    "# --- 1. Simulation Configuration ---\n",
    "D = 4                                  # Dimension of context vectors\n",
    "LAMBDA_VAL = 0.001                     # Baseline hazard constant\n",
    "NUM_CUSTOMERS = 40000                   # Total number of customers to simulate, i.e. T\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "# np.random.seed(41)\n",
    "\n",
    "# Ground truth vectors\n",
    "THETA_TRUE = np.array([0.5, 0.2, 0.4, 0.3])#$, 0.4])    # For degradation\n",
    "UTILITY_TRUE = np.array([0.372450167, 0.10850869, 0.33930126, 0.71356037])\n",
    "\n",
    "# context_sampler()  # For customer's willingness to pay\n",
    "\n",
    "# --- Machine's Pricing Vector 'r' ---\n",
    "# This is a fallback pricing vector, when we don't feed u_hat to calculate_price\n",
    "PRICING_R = np.zeros(D)\n",
    "\n",
    "usage_exp_hazard_model = ExponentialHazard(lambda_val=LAMBDA_VAL)\n",
    "# spontaneous_exp_hazard_model = None # ExponentialHazard(lambda_val=0.01)\n",
    "\n",
    "customer_gen = CustomerGenerator(\n",
    "    d=D,\n",
    "    context_sampler=context_sampler,\n",
    "    rental_sampler=rental_sampler,\n",
    "    interarrival_sampler=interarrival_sampler\n",
    ")\n",
    "\n",
    "centroid_params = {\n",
    "    # 'num_samples': 2000,\n",
    "    # 'thin': None,\n",
    "    # 'burn_in': 500 * D ** 2,\n",
    "    # 'tol': 1e-4,\n",
    "    # 'rho_target': 0.01\n",
    "}\n",
    "\n",
    "termination_rule = lambda diameter: diameter < 0.0005  # Example custom termination rule\n",
    "\n",
    "projected_volume_learner = ProjectedVolumeLearner(\n",
    "    T=NUM_CUSTOMERS, \n",
    "    d=D, \n",
    "    centroid_params=centroid_params,\n",
    "    incentive_constant=1.1,\n",
    "    termination_rule=termination_rule,\n",
    ")\n",
    "\n",
    "mdp_params = {\n",
    "    'duration_lambda': 10.0,\n",
    "    'interarrival_lambda': 5.0,\n",
    "    'replacement_cost': 1.5,   # Cost to replace the machine\n",
    "    'failure_cost': 0.75,      # Additional penalty for in-service failure\n",
    "    'holding_cost_rate': 0.02,   # Cost per unit of idle time\n",
    "    'gamma': 0.99,             # Discount factor\n",
    "    'learning_rate': 1e-3,      # Learning rate for the Adam optimizer\n",
    "    'target_update_freq': 10    # How often to update the target network (in iterations)\n",
    "}\n",
    "\n",
    "training_hyperparams = {\n",
    "    # For FQI\n",
    "    'num_iterations': 1, # Number of training iterations per policy update\n",
    "    'dataset_size': 50000,      # Number of transitions to generate for the offline dataset\n",
    "    'batch_size': 256,           # Batch size for training\n",
    "\n",
    "    # For discrete DP\n",
    "    # 'N': [80, 20, 60, 150], # grid sizes [cum_context, context, duration, active_time\n",
    "    'N': [100, 50, 100, 100], # grid sizes [cum_context, context, revenue, duration]\n",
    "    'max_cumulative_context': 8.0,\n",
    "    # 'max_active_time': 150.0,\n",
    "    'num_value_iterations': 100,\n",
    "    \n",
    "}\n",
    "\n",
    "policy_type = 'decaying_epsilon_greedy'\n",
    "policy_kwargs = {\n",
    "    'current_epsilon': 0.10,\n",
    "    'decay_rate': 0.95,\n",
    "    'step': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_rate(df, time_col, value_col, window_size):\n",
    "    \"\"\"\n",
    "    Calculates the rate of a value over a rolling time window on irregular time series data.\n",
    "    For early time steps, the window size is adaptive, looking back only to time 0.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        time_col (str): The name of the column with time data.\n",
    "        value_col (str): The name of the column with values to aggregate (e.g., 'net_profit').\n",
    "        window_size (int): The maximum duration of the rolling time window.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series containing the calculated rolling rate for each row.\n",
    "    \"\"\"\n",
    "    # Ensure the dataframe is sorted by time, which is crucial.\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    \n",
    "    times = df[time_col].values\n",
    "    values = df[value_col].values\n",
    "    \n",
    "    # --- MODIFICATION 1: Cap the start time at 0 ---\n",
    "    # For each end time `t_i`, find the start time `max(0, t_i - window)`.\n",
    "    start_times = np.maximum(0, times - window_size)\n",
    "    \n",
    "    # Use searchsorted to find the index where each start_time would be inserted.\n",
    "    start_indices = np.searchsorted(times, start_times, side='left')\n",
    "    \n",
    "    # Use a cumulative sum to efficiently calculate the sum over any slice [j, i].\n",
    "    value_cumsum = np.cumsum(values)\n",
    "    shifted_cumsum = np.concatenate(([0], value_cumsum[:-1]))\n",
    "    window_sums = value_cumsum - shifted_cumsum[start_indices]\n",
    "    \n",
    "    # --- MODIFICATION 2: Use the actual window duration as the denominator ---\n",
    "    # The duration is the current time minus the actual start time of the window.\n",
    "    actual_window_durations = times - start_times\n",
    "    \n",
    "    # Initialize profit_rate array to handle potential division by zero\n",
    "    profit_rate = np.zeros_like(times, dtype=float)\n",
    "    \n",
    "    # Create a mask to avoid division by zero where the duration is 0 (e.g., at the very first data point if time=0)\n",
    "    non_zero_duration_mask = actual_window_durations > 0\n",
    "    \n",
    "    # Calculate rate only for non-zero durations\n",
    "    profit_rate[non_zero_duration_mask] = window_sums[non_zero_duration_mask] / actual_window_durations[non_zero_duration_mask]\n",
    "    \n",
    "    return pd.Series(profit_rate, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_utility_updates = []\n",
    "list_of_theta_updates = []\n",
    "for file in os.listdir('models'):\n",
    "    if file.startswith('simulator_') and not file.endswith('discrete_policy.pkl'):\n",
    "        sim = Simulator.load('models/'+file[:-4])\n",
    "        list_of_theta_updates.append(sim.theta_updates)\n",
    "        list_of_utility_updates.append(sim.utility_updates)\n",
    "\n",
    "all_theta_data = []\n",
    "for ind, thta_update in enumerate(list_of_theta_updates):\n",
    "    theta_data = []\n",
    "    for d in thta_update:\n",
    "        idx, diff = d['customer_idx'], d['theta_hat'] - THETA_TRUE\n",
    "        l2, linf= np.linalg.norm(diff, 2), np.linalg.norm(diff, np.inf)\n",
    "        theta_data.append({'customer_idx': idx, 'l2_error': l2, 'linf_error': linf})\n",
    "    all_theta_data.append(pd.DataFrame(theta_data))\n",
    "    \n",
    "all_utility_data = []\n",
    "for ind, util_update in enumerate(list_of_utility_updates):\n",
    "    util_data = []\n",
    "    for d in util_update:\n",
    "        idx, diff = d['customer_idx'], d['u_hat'] - UTILITY_TRUE\n",
    "        l2, linf= np.linalg.norm(diff, 2), np.linalg.norm(diff, np.inf)\n",
    "        util_data.append({'customer_idx': idx, 'l2_error': l2, 'linf_error': linf})\n",
    "    all_utility_data.append(pd.DataFrame(util_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_plot_convergence(ax, all_data, title, color):\n",
    "    \"\"\"\n",
    "    Processes a list of simulation dataframes and plots the smoothed\n",
    "    mean and standard deviation on a given matplotlib axis.\n",
    "\n",
    "    Args:\n",
    "        ax (matplotlib.axes.Axes): The axis object to plot on.\n",
    "        all_data (list): A list of pandas DataFrames from simulation runs.\n",
    "        title (str): The title for the subplot.\n",
    "        color (str): The color to use for the plot lines and shading.\n",
    "    \"\"\"\n",
    "    # --- 1. Resample all trajectories onto a common x-axis ---\n",
    "    min_x = min(df['customer_idx'].min() for df in all_data)\n",
    "    max_x = max(df['customer_idx'].max() for df in all_data)\n",
    "    common_x_axis = np.linspace(min_x, max_x, 500)\n",
    "    \n",
    "    resampled_errors = []\n",
    "    for data in all_data:\n",
    "        resampled_error = np.interp(\n",
    "            common_x_axis,\n",
    "            data['customer_idx'],\n",
    "            data['l2_error']\n",
    "        )\n",
    "        resampled_errors.append(resampled_error)\n",
    "\n",
    "    error_matrix = np.vstack(resampled_errors)\n",
    "    mean_error = np.mean(error_matrix, axis=0)\n",
    "    std_error = np.std(error_matrix, axis=0)\n",
    "\n",
    "    # --- 2. Smoothen the curves ---\n",
    "    smoothing_window = 20\n",
    "    smooth_mean_error = pd.Series(mean_error).rolling(window=smoothing_window, center=True, min_periods=1).mean()\n",
    "    smooth_std_error = pd.Series(std_error).rolling(window=smoothing_window, center=True, min_periods=1).mean()\n",
    "\n",
    "    # --- 3. Plot on the provided axis ---\n",
    "    ax.plot(common_x_axis, smooth_mean_error, label='Mean $\\ell^2$ Norm Error', color=color)\n",
    "    ax.fill_between(\n",
    "        common_x_axis,\n",
    "        smooth_mean_error - smooth_std_error,\n",
    "        smooth_mean_error + smooth_std_error,\n",
    "        color=color,\n",
    "        alpha=0.2,\n",
    "        label='1 Standard Deviation'\n",
    "    )\n",
    "    \n",
    "    # set tick font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    \n",
    "    ax.set_xlabel('Number of Customers Processed', fontsize=24)\n",
    "    ax.set_ylabel('Error Norm', fontsize=24)\n",
    "    ax.set_title(title, fontsize=28)\n",
    "    ax.legend(fontsize=18)\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "# --- Main plotting logic ---\n",
    "\n",
    "# Assume 'all_utility_data' and 'all_theta_data' are loaded\n",
    "# For example:\n",
    "# all_utility_data = [pd.read_csv(f) for f in utility_files]\n",
    "# all_theta_data = [pd.read_csv(f) for f in theta_files]\n",
    "\n",
    "# Create a figure with 2 rows and 1 column of subplots.\n",
    "# `figsize` controls the overall size of the combined image.\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# Plot the first dataset on the top subplot (axes[0])\n",
    "process_and_plot_convergence(\n",
    "    ax=axes[0], \n",
    "    all_data=all_utility_data, \n",
    "    title=r'Convergence of $\\|\\hat u - u\\|$', \n",
    "    color='royalblue'\n",
    ")\n",
    "\n",
    "plot_theta_data = [df[df.customer_idx <= 20000].reset_index(drop=True).copy() for df in all_theta_data]\n",
    "\n",
    "# Plot the second dataset on the bottom subplot (axes[1])\n",
    "process_and_plot_convergence(\n",
    "    ax=axes[1], \n",
    "    all_data=plot_theta_data, \n",
    "    title=r'Convergence of $\\|\\hat \\theta - \\theta\\|$', \n",
    "    color='forestgreen'\n",
    ")\n",
    "\n",
    "# Adjust the layout to prevent titles and labels from overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the combined figure\n",
    "plt.savefig('figures/combined_convergence.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22053bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_theta_data[0]\n",
    "\n",
    "df[df.customer_idx <= 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84661c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_duration = 10000 # Define the time window for the rolling rate\n",
    "\n",
    "simulation_dfs = []\n",
    "# use os module to find all csv files starting with 'data/simulation_data_'\n",
    "for file in os.listdir('data'):\n",
    "    if file.startswith('simulation_data_') and file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join('data', file))\n",
    "        df['net_profit'] = df['profit'] + df['loss']\n",
    "        df['profit_rate'] = calculate_rolling_rate(df, 'calendar_time', 'net_profit', window_duration)\n",
    "        simulation_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dfs = pd.read_csv('data/optimal_trajectories.csv')\n",
    "\n",
    "optimal_rev_dfs = []\n",
    "for i in optimal_dfs.run_id.unique():\n",
    "    df = optimal_dfs[optimal_dfs.run_id == i].reset_index(drop=True).copy()\n",
    "    df['calendar_time'] = df['calendar_time'] - df['calendar_time'].min()  # Normalize time to start from 0\n",
    "    df['net_profit'] = df['profit'] + df['loss']\n",
    "    df['profit_rate'] = calculate_rolling_rate(df, 'calendar_time', 'net_profit', window_duration)\n",
    "    optimal_rev_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30933eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the overall time range for the common grid\n",
    "min_time = min(df['calendar_time'].min() for df in simulation_dfs)\n",
    "\n",
    "min_time = 25\n",
    "max_time = min(\n",
    "            max(df['calendar_time'].max() for df in simulation_dfs),\n",
    "            max(df['calendar_time'].max() for df in optimal_rev_dfs)\n",
    "        )\n",
    "max_time = 350000\n",
    "\n",
    "# Create a uniform time grid. 500 points is usually a good resolution.\n",
    "common_time_grid = np.linspace(min_time, max_time, 500)\n",
    "\n",
    "# A list to store the profit rate of each run, interpolated onto the common grid\n",
    "online_resampled_profit_rates = []\n",
    "for sim_df in simulation_dfs:\n",
    "    # For each simulation, interpolate its profit_rate onto the common_time_grid\n",
    "    # np.interp(new_x, old_x, old_y)\n",
    "    resampled_rate = np.interp(\n",
    "        common_time_grid, \n",
    "        sim_df['calendar_time'], \n",
    "        sim_df['profit_rate']\n",
    "    )\n",
    "    online_resampled_profit_rates.append(resampled_rate)\n",
    "# --- 2. Calculate the mean and standard deviation across all runs ---\n",
    "# Convert the list of arrays into a 2D NumPy array for easy computation\n",
    "# Each row is a simulation run, each column is a time point\n",
    "online_profit_rate_matrix = np.vstack(online_resampled_profit_rates)\n",
    "# Calculate the mean and standard deviation down each column (axis=0)\n",
    "online_mean_profit_rate = np.mean(online_profit_rate_matrix, axis=0)\n",
    "online_std_profit_rate = np.std(online_profit_rate_matrix, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# A list to store the profit rate of each run, interpolated onto the common grid\n",
    "optimal_resampled_profit_rates = []\n",
    "for sim_df in optimal_rev_dfs:\n",
    "    # For each simulation, interpolate its profit_rate onto the common_time_grid\n",
    "    # np.interp(new_x, old_x, old_y)\n",
    "    resampled_rate = np.interp(\n",
    "        common_time_grid, \n",
    "        sim_df['calendar_time'], \n",
    "        sim_df['profit_rate']\n",
    "    )\n",
    "    optimal_resampled_profit_rates.append(resampled_rate)\n",
    "# --- 2. Calculate the mean and standard deviation across all runs ---\n",
    "# Convert the list of arrays into a 2D NumPy array for easy computation\n",
    "# Each row is a simulation run, each column is a time point\n",
    "optimal_profit_rate_matrix = np.vstack(optimal_resampled_profit_rates)\n",
    "# Calculate the mean and standard deviation down each column (axis=0)\n",
    "optimal_mean_profit_rate = np.mean(optimal_profit_rate_matrix, axis=0)\n",
    "optimal_std_profit_rate = np.std(optimal_profit_rate_matrix, axis=0)\n",
    "\n",
    "\n",
    "# --- 3. Plot the average trajectory with the shaded standard deviation ---\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot the average trajectory\n",
    "plt.plot(common_time_grid, online_mean_profit_rate, label='Mean Online Learning Profit Rate', color='blue')\n",
    "\n",
    "# Add the shaded area for the standard deviation (mean +/- 1 std)\n",
    "plt.fill_between(\n",
    "    common_time_grid,\n",
    "    online_mean_profit_rate - online_std_profit_rate,\n",
    "    online_mean_profit_rate + online_std_profit_rate,\n",
    "    color='blue',\n",
    "    alpha=0.2, # Use transparency to make it look nice\n",
    "    label='1 Standard Deviation'\n",
    ")\n",
    "\n",
    "# Plot the average trajectory\n",
    "plt.plot(common_time_grid, optimal_mean_profit_rate, label='Mean Optimal Learning Profit Rate', color='red')\n",
    "\n",
    "# Add the shaded area for the standard deviation (mean +/- 1 std)\n",
    "plt.fill_between(\n",
    "    common_time_grid,\n",
    "    optimal_mean_profit_rate - optimal_std_profit_rate,\n",
    "    optimal_mean_profit_rate + optimal_std_profit_rate,\n",
    "    color='red',\n",
    "    alpha=0.2, # Use transparency to make it look nice\n",
    "    label='1 Standard Deviation'\n",
    ")\n",
    "\n",
    "plt.xlabel('Time', fontsize=18)\n",
    "plt.ylabel('Profit Rate (Profit / Time Unit)', fontsize=18)\n",
    "\n",
    "plt.ylim(top=0.015, bottom=-0.015)\n",
    "\n",
    "# set x, y axes font size\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# insert horizontal line at y=0\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title(f'Mean Rolling Profit Rate', fontsize=22)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.savefig('figures/average_profit_rate_comparison.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71d0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7947b88a",
   "metadata": {},
   "source": [
    "### Plotting optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerfectDegradationLearner:\n",
    "    def __init__(self, d, theta_true, hazard_model):\n",
    "        self.d = d\n",
    "        self.theta_true = theta_true\n",
    "        self.hazard_model = hazard_model  # Placeholder, not used\n",
    "        \n",
    "    def get_theta(self):\n",
    "        return self.theta_true\n",
    "    \n",
    "    def cum_baseline(self, t):\n",
    "        return self.hazard_model.Lambda_0(t)\n",
    "    \n",
    "    def inverse_cum_baseline(self, u):\n",
    "        return self.hazard_model.Lambda_0_inverse(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbfebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect_dpagent.run_value_iteration(100)\n",
    "# perfect_dpagent.save_policy('models/perfect_discrete_policy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_dpagent = DiscretizedDPAgent.load_policy('models/perfect_discrete_policy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ccdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_arrival_decision_grid(ddpa, fixed_customer_context, grid_resolution=75):\n",
    "    \"\"\"\n",
    "    Computes the deterministic action for a grid of cumulative context vs. duration.\n",
    "    (This function remains unchanged)\n",
    "    \"\"\"\n",
    "    cumulative_context_grid = np.linspace(0, ddpa.grid_max_vals[0], grid_resolution)\n",
    "    duration_grid = np.linspace(0, ddpa.grid_max_vals[3], grid_resolution)\n",
    "    action_grid = np.zeros((grid_resolution, grid_resolution), dtype=int)\n",
    "    \n",
    "    for i, cum_context in enumerate(cumulative_context_grid):\n",
    "        for j, duration in enumerate(duration_grid):\n",
    "            state_tuple = (cum_context, 0, fixed_customer_context, duration)\n",
    "            state_indices = ddpa._get_state_indices(state_tuple)\n",
    "            action_grid[j, i] = ddpa.policy_arrival[state_indices]\n",
    "            \n",
    "    return action_grid, cumulative_context_grid, duration_grid\n",
    "\n",
    "# --- Main Plotting Logic ---\n",
    "\n",
    "# 1. Pre-calculate the customer context values for percentiles\n",
    "sampled_customers = [customer_gen.generate() for _ in range(10000)]\n",
    "sampled_cx = np.array([np.dot(perfect_dpagent.theta, c['context']) for c in sampled_customers])\n",
    "percentiles_to_plot = [10, 50, 90]\n",
    "cx_percentiles = {p: np.percentile(sampled_cx, p) for p in percentiles_to_plot}\n",
    "\n",
    "# 2. Create a figure with 1 row and 3 columns of subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7), sharey=True)\n",
    "\n",
    "# --- CHANGE 1: Define a Purple/Yellow binary colormap ---\n",
    "cmap = mcolors.ListedColormap(['#440154', '#FDE725']) # Purple, Yellow\n",
    "bounds = [-0.5, 0.5, 1.5]\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# 4. Generate and plot a heatmap for each percentile\n",
    "for i, p in enumerate(percentiles_to_plot):\n",
    "    ax = axes[i]\n",
    "    fixed_cx = cx_percentiles[p]\n",
    "    \n",
    "    decision_grid, context_axis, duration_axis = compute_arrival_decision_grid(\n",
    "        perfect_dpagent, \n",
    "        fixed_customer_context=fixed_cx, \n",
    "        grid_resolution=100\n",
    "    )\n",
    "\n",
    "    im = ax.imshow(decision_grid, origin='lower', aspect='auto', cmap=cmap, norm=norm,\n",
    "                   extent=[context_axis[0], context_axis[-1],\n",
    "                           duration_axis[0], duration_axis[-1]])\n",
    "    \n",
    "    ax.set_xlabel(r'Cumulative Context ($\\theta^T X$)', fontsize=20)\n",
    "    ax.set_title(f'Decision Boundary\\n($u^T x$ at {p}th percentile)', fontsize=20)\n",
    "\n",
    "axes[0].set_ylabel('Desired Rental Duration (T)', fontsize=20)\n",
    "fig.suptitle('Arrival Policy Decision Boundaries', fontsize=28, y=1.03)\n",
    "\n",
    "\n",
    "# --- CHANGE 2: Manually position the colorbar outside the plots ---\n",
    "\n",
    "# Adjust subplot parameters to make space on the right for the colorbar\n",
    "fig.subplots_adjust(right=0.88)\n",
    "\n",
    "# Add a new axis for the colorbar [left, bottom, width, height] in figure coordinates\n",
    "cbar_ax = fig.add_axes([0.9, 0.15, 0.02, 0.7])\n",
    "\n",
    "# Create the colorbar on the new axis\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, ticks=[0, 1])\n",
    "cbar.ax.set_yticklabels(['Accept', 'Reject'], fontsize=18)\n",
    "# cbar.set_label('Action', fontsize=16)\n",
    "\n",
    "# --- MODIFICATION: Use tight_layout to adjust spacing ---\n",
    "# This will automatically adjust spacing to prevent overlap,\n",
    "# including the suptitle. The rect parameter leaves space for the suptitle and colorbar.\n",
    "fig.tight_layout(rect=[0, 0, 0.9, 1.02]) # rect=[left, bottom, right, top]\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('figures/arrival_policy_side_by_side.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535697aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_departure_policy_threshold(ddpa):\n",
    "    \"\"\"\n",
    "    Generates a plot showing the departure policy threshold (Replace vs. No Replace).\n",
    "\n",
    "    The plot shows the decision for each value of cumulative context,\n",
    "    while keeping the machine's active time fixed.\n",
    "\n",
    "    Args:\n",
    "        ddpa: The trained DiscretizedDPAgent object.\n",
    "        fixed_active_time_idx (int): The grid index for the fixed active time (t_i).\n",
    "    \"\"\"\n",
    "    # Extract the relevant 1D slice of the policy\n",
    "    policy_slice = ddpa.policy_departure\n",
    "    \n",
    "    # The actions are 2 (replace) and 3 (no_replace). We map them to 0 and 1 for plotting.\n",
    "    decision = np.where(policy_slice == 2, 1, 0)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(16, 7.5))\n",
    "    plt.step(ddpa.grids[0], decision, where='post')\n",
    "    \n",
    "    # --- Formatting ---\n",
    "    plt.xlabel(r'Cumulative Context ($\\theta^T X$)', fontsize=28)\n",
    "    # plt.ylabel('Decision', fontsize=18)\n",
    "    plt.title(f'Departure Policy Threshold', fontsize=40)\n",
    "    plt.yticks([0, 1], ['Continue', 'Replace'])\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    # set tick font size\n",
    "    plt.tick_params(axis='both', which='major', labelsize=28)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.savefig('figures/departure_policy_threshold.pdf', bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 2. Visualize the DEPARTURE policy\n",
    "print(\"\\n--- Generating Departure Policy Plot ---\")\n",
    "# Let's look at the replacement threshold for a machine of average age\n",
    "mid_active_time_idx = len(perfect_dpagent.grids[3]) // 2\n",
    "plot_departure_policy_threshold(perfect_dpagent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f50b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a615b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
